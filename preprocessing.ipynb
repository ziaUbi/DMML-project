{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lavoro sui dataset iniziali con tutte le etichette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.feature_selection import SelectKBest, chi2, f_classif, mutual_info_classif\n",
    "from sklearn.preprocessing import TargetEncoder\n",
    "\n",
    "import sys\n",
    "sys.path.insert(1, '')\n",
    "from utility import Dataset\n",
    "\n",
    "train_df = pd.read_csv(f'KDDTrain+.txt', header=None)\n",
    "test_df = pd.read_csv(f'KDDTest+.txt', header=None)\n",
    "\n",
    "train_df = Dataset(train_df)\n",
    "test_df = Dataset(test_df)\n",
    "\n",
    "# nominal_features = ['protocol_type', 'service', 'flag']\n",
    "# binary_features = ['land', 'logged_in', 'root_shell', 'su_attempted', 'is_host_login', 'is_guest_login']\n",
    "# numeric_features = [feature for feature in columns if feature not in nominal_features + binary_features + ['label', 'score']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** remove highly uncorrelated features ?? **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding of categorical variables\n",
    "One hot encoder per 'protocol type' (3 valori) e 'flag' (11 valori) \\\n",
    "Target encoder per 'service' (70 valori) \\\n",
    "ATTENZIONE: se ci dovesse essere overfitting, sostituire Target encoder con Frequency encoder \\\n",
    "oppure fare smoothing o cross-validation (chatGPT spiega come si fa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['duration', 'src_bytes', 'dst_bytes', 'land', 'wrong_fragment',\n",
      "       'urgent', 'hot', 'num_failed_logins', 'logged_in', 'num_compromised',\n",
      "       'root_shell', 'su_attempted', 'num_root', 'num_file_creations',\n",
      "       'num_shells', 'num_access_files', 'is_host_login', 'is_guest_login',\n",
      "       'count', 'srv_count', 'serror_rate', 'srv_serror_rate', 'rerror_rate',\n",
      "       'srv_rerror_rate', 'same_srv_rate', 'diff_srv_rate',\n",
      "       'srv_diff_host_rate', 'dst_host_count', 'dst_host_srv_count',\n",
      "       'dst_host_same_srv_rate', 'dst_host_diff_srv_rate',\n",
      "       'dst_host_same_src_port_rate', 'dst_host_srv_diff_host_rate',\n",
      "       'dst_host_serror_rate', 'dst_host_srv_serror_rate',\n",
      "       'dst_host_rerror_rate', 'dst_host_srv_rerror_rate', 'label', 'score',\n",
      "       'service', 'protocol_type_icmp', 'protocol_type_tcp',\n",
      "       'protocol_type_udp', 'flag_OTH', 'flag_REJ', 'flag_RSTO', 'flag_RSTOS0',\n",
      "       'flag_RSTR', 'flag_S0', 'flag_S1', 'flag_S2', 'flag_S3', 'flag_SF',\n",
      "       'flag_SH'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "def oh_encoder(train_df, test_df, nominal_features):\n",
    "    enc = OneHotEncoder()\n",
    "    train_encoded = enc.fit_transform(train_df[nominal_features]).toarray()\n",
    "    test_encoded = enc.transform(test_df[nominal_features]).toarray()\n",
    "    new_columns = []\n",
    "    for i, feature in enumerate(nominal_features):\n",
    "        new_columns.extend([f\"{feature}_{str(cat)}\" for cat in enc.categories_[i]])\n",
    "\n",
    "    train_ohe = train_df.drop(nominal_features, axis=1)\n",
    "    train_ohe = pd.concat([train_ohe, pd.DataFrame(train_encoded, columns=new_columns)], axis=1)\n",
    "\n",
    "    test_ohe = test_df.drop(nominal_features, axis=1)\n",
    "    test_ohe = pd.concat([test_ohe, pd.DataFrame(test_encoded, columns=new_columns)], axis=1)\n",
    "\n",
    "    return train_ohe, test_ohe\n",
    "\n",
    "def t_encoder(train_df, test_df, nominal_features):\n",
    "    enc = TargetEncoder()\n",
    "    train_encoded = enc.fit_transform(train_df[nominal_features], train_df['label'])\n",
    "    test_encoded = enc.transform(test_df[nominal_features])\n",
    "\n",
    "    train_t = train_df.drop(nominal_features, axis=1)\n",
    "    train_t = pd.concat([train_t, pd.DataFrame(train_encoded, columns=nominal_features)], axis=1)\n",
    "\n",
    "    test_t = test_df.drop(nominal_features, axis=1)\n",
    "    test_t = pd.concat([test_t, pd.DataFrame(test_encoded, columns=nominal_features)], axis=1)\n",
    "\n",
    "    return train_t, test_t\n",
    "\n",
    "# train_new = Dataset(train_df).get_label2()\n",
    "# test_new= Dataset(test_df).get_label2()\n",
    "# train_t, test_t = t_encoder(train_new, test_new, ['service', ])\n",
    "# train_t, test_t = oh_encoder(train_t, test_t, ['protocol_type', 'flag'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per le feature continue: Se queste seguono o sono vicine a una distribuzione normale, \n",
    "# lo StandardScaler è spesso una buona scelta. Se invece ci sono outlier o i dati \n",
    "# sono distribuiti in modo non normale, considera RobustScaler o MinMaxScaler. (CONTROLLARE GLI OUTLIERS)\n",
    "# Per le feature categoriali codificate (come One-Hot Encoded): in generale, \n",
    "# non è necessario applicare uno scaler poiché i valori saranno già binari (0 e 1). \n",
    "# Tuttavia, se hai usato un encoding come il Target Encoding, potresti voler applicare \n",
    "# uno scaler come MinMaxScaler per portare i valori target in un intervallo uniforme.\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def scaler(train_df, test_df, continuous_features, scaler = MinMaxScaler()):\n",
    "    train_scaled = scaler.fit_transform(train_df[continuous_features])\n",
    "    test_scaled = scaler.transform(test_df[continuous_features])\n",
    "\n",
    "    train_ss = train_df.drop(continuous_features, axis=1)\n",
    "    train_ss = pd.concat([train_ss, pd.DataFrame(train_scaled, columns=continuous_features)], axis=1)\n",
    "\n",
    "    test_ss = test_df.drop(continuous_features, axis=1)\n",
    "    test_ss = pd.concat([test_ss, pd.DataFrame(test_scaled, columns=continuous_features)], axis=1)\n",
    "\n",
    "    return train_ss, test_ss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection \n",
    "\n",
    "1. Attribute Ratio\n",
    "\n",
    "2. Correlation-based Feature Selection (chi-square, mutual information)\n",
    "\n",
    "3. Information Gain\n",
    "\n",
    "4. Gain Ratio\n",
    "\n",
    "5. PCA\n",
    "\n",
    "6. RFE\n",
    "\n",
    "7. SFS\n",
    "\n",
    "8. Variance Threshold ma solo se ho tempo\n",
    "\n",
    "# 1. Attribute Ratio\n",
    "Attribute Ratio approach is used for feature selection purposes. This approach was described by Hee-su Chae and Sang Hyun Choi in Feature Selection for efficient Intrusion Detection using Attribute Ratio and Feature Selection for Intrusion Detection using NSL-KDD\n",
    "\n",
    "This approach is also used for nominal variables as they were encoded as binary variables above.\n",
    "\n",
    "As it is a possible to have 'null' values because binary features could have Frequency(0) = 0, those 'null' values are replaced with 1000.0 (magic number). For NSL KDD dataset it is related only for 'protocol_type_tcp' ohe variable.\n",
    "\n",
    "In section 4, we explain NSL-KDD data which has three attribute types. We use attribute average and frequency for each class calculate the AR from numeric and binary type. AR can be calculated as :                             (7)   Class Ratio (CR) is attribute is ratio of each class for Attribute i. CR is calculated by two methods according to the type of attributes. CR can be calculated as for numeric :                                      (8)  CR can be calculated as for binary : \n",
    "\n",
    "After calculating AR(i), Features rank ordering larger AR. Table 4 shows the rank of features with a calculated AR. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 & num_shells & 326.11 \\\\\n",
      "2 & urgent & 173.04 \\\\\n",
      "3 & num_file_creations & 62.23 \\\\\n",
      "4 & num_failed_logins & 46.04 \\\\\n",
      "5 & hot & 40.77 \\\\\n",
      "6 & logged_in & 10.57 \\\\\n",
      "7 & dst_bytes & 9.15 \\\\\n",
      "8 & src_bytes & 8.46 \\\\\n",
      "9 & duration & 7.23 \\\\\n",
      "10 & dst_host_srv_diff_host_rate & 5.76 \\\\\n",
      "11 & dst_host_diff_srv_rate & 4.84 \\\\\n",
      "12 & num_access_files & 4.69 \\\\\n",
      "13 & dst_host_same_src_port_rate & 4.39 \\\\\n",
      "14 & num_compromised & 4.34 \\\\\n",
      "15 & diff_srv_rate & 4.07 \\\\\n",
      "16 & dst_host_srv_rerror_rate & 3.67 \\\\\n",
      "17 & srv_rerror_rate & 3.67 \\\\\n",
      "18 & rerror_rate & 3.65 \\\\\n",
      "19 & dst_host_rerror_rate & 3.28 \\\\\n",
      "20 & srv_diff_host_rate & 3.08 \\\\\n",
      "21 & wrong_fragment & 2.74 \\\\\n",
      "22 & dst_host_srv_serror_rate & 2.67 \\\\\n",
      "23 & srv_serror_rate & 2.64 \\\\\n",
      "24 & serror_rate & 2.63 \\\\\n",
      "25 & dst_host_serror_rate & 2.63 \\\\\n",
      "26 & num_root & 2.61 \\\\\n",
      "27 & count & 2.12 \\\\\n",
      "28 & dst_host_srv_count & 1.65 \\\\\n",
      "29 & dst_host_same_srv_rate & 1.56 \\\\\n",
      "30 & same_srv_rate & 1.51 \\\\\n",
      "31 & dst_host_count & 1.34 \\\\\n",
      "32 & srv_count & 1.18 \\\\\n",
      "33 & root_shell & 1.00 \\\\\n",
      "34 & is_guest_login & 0.46 \\\\\n",
      "35 & su_attempted & 0.00 \\\\\n",
      "36 & land & 0.00 \\\\\n",
      "37 & is_host_login & 0.00 \\\\\n",
      "38 & protocol_type & 0.00 \\\\\n",
      "39 & service & 0.00 \\\\\n",
      "40 & flag & 0.00 \\\\\n"
     ]
    }
   ],
   "source": [
    "# ho calcolato l'attribute ratio per ogni feature (divisione in 5 classi)\n",
    "train_5_df = Dataset(train_df).get_label5()\n",
    "\n",
    "def attributeRatio(feature, dataset):\n",
    "    class_ratio = {}\n",
    "\n",
    "    if feature in nominal_features:\n",
    "        return 0\n",
    "\n",
    "    if feature in binary_features:\n",
    "        for label in dataset['label'].unique():\n",
    "            class_ones = dataset[dataset['label'] == label][feature].sum()\n",
    "            class_zeros = dataset[dataset['label'] == label][feature].count() - class_ones\n",
    "            class_ratio[label] = class_ones / class_zeros\n",
    "    else:\n",
    "        total_mean = dataset[feature].mean()\n",
    "        for label in dataset['label'].unique():\n",
    "            class_mean = dataset[dataset['label'] == label][feature].mean()\n",
    "            class_ratio[label] = class_mean / total_mean\n",
    "    return max(class_ratio.values())\n",
    "\n",
    "# calcola l'attribute ratio per ogni feature tranne 'label' e 'score'\n",
    "attribute_ratios = {}\n",
    "for feature in train_5_df.columns:\n",
    "    if feature not in ['label', 'score', 'num_outbound_cmds']:\n",
    "        attribute_ratios[feature] = attributeRatio(feature, train_5_df)\n",
    "\n",
    "# ordina le feature in base all'attribute ratio\n",
    "sorted_attribute_ratios = sorted(attribute_ratios.items(), key=lambda x: x[1], reverse=True)\n",
    "# stampa una tabella con le feature ordinate. la prima colonna è il rank, la seconda la feature e la terza l'attribute ratio\n",
    "for i, (feature, ratio) in enumerate(sorted_attribute_ratios):\n",
    "    print(f\"{i+1} & {feature} & {ratio:.2f} \\\\\\\\\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Correlation-based Feature Selection\n",
    "https://medium.com/@sariq16/correlation-based-feature-selection-in-a-data-science-project-3ca08d2af5c6\n",
    "\n",
    "seleziona le k feature più rilevanti secondo il chi2 (hanno alta correlazione con il label e bassa con le altre feature), ANOVA F-test, Mutual Information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(125973, 10)\n",
      "(125973, 10)\n",
      "(125973, 10)\n"
     ]
    }
   ],
   "source": [
    "def get_best_features(train_data, test_data, score_func, k):\n",
    "    X = train_data.drop(['label', 'score'], axis=1)\n",
    "    y = train_data['label']\n",
    "    selector = SelectKBest(score_func=score_func, k = k).fit(X, y)\n",
    "    X_train_selected = selector.transform(X)\n",
    "    X_test_selected = selector.transform(test_data.drop(['label', 'score'], axis=1))\n",
    "    return X_train_selected, X_test_selected\n",
    "\n",
    "\n",
    "train_new = Dataset(train_df).get_label2()\n",
    "test_new= Dataset(test_df).get_label2()\n",
    "train_t, test_t = t_encoder(train_new, test_new, ['service', ])\n",
    "train_t, test_t = oh_encoder(train_t, test_t, ['protocol_type', 'flag'])\n",
    "\n",
    "train_reduced, test_reduced = get_best_features(train_t, test_t, f_classif, 10)\n",
    "print(train_reduced.shape)\n",
    "train_reduced, test_reduced = get_best_features(train_t, test_t, chi2, 10)\n",
    "print(train_reduced.shape)\n",
    "train_reduced, test_reduced = get_best_features(train_t, test_t, mutual_info_classif, 10)\n",
    "print(train_reduced.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Wrapper Methods\n",
    "\n",
    "Questi metodi iterano attraverso le combinazioni di feature e valutano la loro importanza usando un modello predittivo.\n",
    "\n",
    "Recursive Feature Elimination (RFE): Utilizza un modello per selezionare feature rilevanti, eliminando quelle meno importanti ad ogni iterazione. Funziona bene con modelli come SVM, alberi decisionali o regressioni logistiche."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "train_new = Dataset(train_df).get_label2()\n",
    "test_new= Dataset(test_df).get_label2()\n",
    "train_rfe, test_rfe = t_encoder(train_new, test_new, ['service', ])\n",
    "train_rfe, test_rfe = oh_encoder(train_rfe, test_rfe, ['protocol_type', 'flag'])\n",
    "\n",
    "X_train_rfe = train_rfe.drop(['label', 'score'], axis=1)\n",
    "y_train_rfe = train_rfe['label']\n",
    "X_test_rfe = test_rfe.drop(['label', 'score'], axis=1)\n",
    "y_test_rfe = test_rfe['label']\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "rfe = RFE(model, n_features_to_select=10).fit(X_train_rfe, y_train_rfe)\n",
    "X_train_rfe_selected = rfe.transform(X_train_rfe)\n",
    "X_test_rfe_selected = rfe.transform(X_test_rfe)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequential Feature Selection (SFS): Aggiunge o rimuove feature in modo sequenziale, valutando il modello in ogni passaggio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, random_state=123, shuffle=True)\n",
    "\n",
    "train_new = Dataset(train_df).get_label2()\n",
    "test_new= Dataset(test_df).get_label2()\n",
    "train_sfs, test_sfs = t_encoder(train_new, test_new, ['service', ])\n",
    "train_sfs, test_sfs = oh_encoder(train_sfs, test_sfs, ['protocol_type', 'flag'])\n",
    "\n",
    "X_train_sfs = train_sfs.drop(['label', 'score'], axis=1)\n",
    "y_train_sfs = train_sfs['label']\n",
    "X_test_sfs = test_sfs.drop(['label', 'score'], axis=1)\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "sfs = SequentialFeatureSelector(model, \n",
    "                                cv = skf,\n",
    "                                scoring = 'accuracy', \n",
    "                                direction='forward', \n",
    "                                n_features_to_select=10).fit(X_train_rfe, y_train_rfe)\n",
    "X_train_sfs_selected = sfs.transform(X_train_sfs)\n",
    "X_test_sfs_selected = sfs.transform(X_test_sfs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calcola information gain \n",
    "def get_information_gain(dataset, feature):\n",
    "    entropy = 0\n",
    "    for label in dataset['label'].unique():\n",
    "        p = dataset[dataset['label'] == label][feature].sum() / dataset[feature].sum()\n",
    "        entropy += p * np.log2(p)\n",
    "    return entropy\n",
    "\n",
    "information_gains = {}\n",
    "for feature in train_5_df.columns:\n",
    "    if feature not in ['label', 'score', 'num_outbound_cmds']:\n",
    "        information_gains[feature] = get_information_gain(train_5_df, feature)\n",
    "\n",
    "# ordina le feature in base all'information gain\n",
    "sorted_information_gains = sorted(information_gains.items(), key=lambda x: x[1], reverse=True)\n",
    "# stampa una tabella con le feature ordinate. la prima colonna è il rank, la seconda la feature e la terza l'information gain\n",
    "for i, (feature, gain) in enumerate(sorted_information_gains):\n",
    "    print(f\"{i+1} & {feature} & {gain:.2f} \\\\\\\\\")\n",
    "\n",
    "\n",
    "# calcola gain ratio\n",
    "def get_gain_ratio(dataset, feature):\n",
    "    split_info = 0\n",
    "    for label in dataset['label'].unique():\n",
    "        p = dataset[dataset['label'] == label][feature].sum() / dataset[feature].sum()\n",
    "        split_info += p * np.log2(p)\n",
    "    split_info = -split_info\n",
    "    return information_gains[feature] / split_info\n",
    "\n",
    "gain_ratios = {}\n",
    "for feature in train_5_df.columns:\n",
    "    if feature not in ['label', 'score', 'num_outbound_cmds']:\n",
    "        gain_ratios[feature] = get_gain_ratio(train_5_df, feature)\n",
    "\n",
    "# ordina le feature in base al gain ratio\n",
    "sorted_gain_ratios = sorted(gain_ratios.items(), key=lambda x: x[1], reverse=True)\n",
    "# stampa una tabella con le feature ordinate. la prima colonna è il rank, la seconda la feature e la terza il gain ratio\n",
    "for i, (feature, ratio) in enumerate(sorted_gain_ratios):\n",
    "    print(f\"{i+1} & {feature} & {ratio:.2f} \\\\\\\\\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "train_new = Dataset(train_df).get_label2()\n",
    "X_train_pca = train_new.drop(['label', 'score'], axis=1)\n",
    "y_train_pca = train_new['label']\n",
    "X_test_pca = test_df.drop(['label', 'score'], axis=1)\n",
    "\n",
    "pca = PCA(n_components='mle')\n",
    "X_train_pca = pca.fit_transform(X_train_pca)\n",
    "X_test_pca = pca.transform(X_test_pca)\n",
    "print(X_train_pca.shape)\n",
    "print(pca.components_)\n",
    "print(pca.explained_variance_ratio_)\n",
    "print(pca.explained_variance_)\n",
    "print(pca.singular_variance_ratio_.sum()*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.Discretization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envTesi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
