{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lavoro sui dataset iniziali con tutte le etichette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import TargetEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_selection import SelectKBest, chi2, f_classif, mutual_info_classif\n",
    "\n",
    "import sys\n",
    "sys.path.insert(1, '')\n",
    "from utility import Dataset\n",
    "\n",
    "columns = ['duration', 'protocol_type', 'service', 'flag', 'src_bytes', 'dst_bytes', 'land', 'wrong_fragment',\n",
    "           'urgent', 'hot', 'num_failed_logins', 'logged_in', 'num_compromised', 'root_shell', 'su_attempted',\n",
    "           'num_root', 'num_file_creations', 'num_shells', 'num_access_files', 'num_outbound_cmds', 'is_host_login',\n",
    "           'is_guest_login', 'count', 'srv_count', 'serror_rate', 'srv_serror_rate', 'rerror_rate', 'srv_rerror_rate',\n",
    "           'same_srv_rate', 'diff_srv_rate', 'srv_diff_host_rate', 'dst_host_count', 'dst_host_srv_count',\n",
    "           'dst_host_same_srv_rate', 'dst_host_diff_srv_rate', 'dst_host_same_src_port_rate',\n",
    "           'dst_host_srv_diff_host_rate', 'dst_host_serror_rate', 'dst_host_srv_serror_rate', 'dst_host_rerror_rate',\n",
    "           'dst_host_srv_rerror_rate', 'label', 'score']\n",
    "\n",
    "nominal_features = ['protocol_type', 'service', 'flag']\n",
    "binary_features = ['land', 'logged_in', 'root_shell', 'su_attempted', 'is_host_login', 'is_guest_login']\n",
    "numeric_features = [feature for feature in columns if feature not in nominal_features + binary_features + ['label', 'score', 'num_outbound_cmds']]\n",
    "\n",
    "train_df = pd.read_csv(f'dataset/nsl-kdd/KDDTrain+.txt', header=None)\n",
    "test_df = pd.read_csv(f'dataset/nsl-kdd/KDDTest+.txt', header=None)\n",
    "\n",
    "train_df = Dataset(train_df, columns).get_label5()\n",
    "test_df = Dataset(test_df, columns).get_label5()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding of categorical variables\n",
    "One hot encoder per 'protocol type' (3 valori) e 'flag' (11 valori) \\\n",
    "Target encoder per 'service' (70 valori) \\\n",
    "ATTENZIONE: se ci dovesse essere overfitting, sostituire Target encoder con Frequency encoder \\\n",
    "oppure fare smoothing o cross-validation (chatGPT spiega come si fa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        duration  protocol_type  service  flag  src_bytes  dst_bytes  land  \\\n",
      "0              0              1       20     9        491          0     0   \n",
      "1              0              2       44     9        146          0     0   \n",
      "2              0              1       49     5          0          0     0   \n",
      "3              0              1       24     9        232       8153     0   \n",
      "4              0              1       24     9        199        420     0   \n",
      "...          ...            ...      ...   ...        ...        ...   ...   \n",
      "125968         0              1       49     5          0          0     0   \n",
      "125969         8              2       49     9        105        145     0   \n",
      "125970         0              1       54     9       2231        384     0   \n",
      "125971         0              1       30     5          0          0     0   \n",
      "125972         0              1       20     9        151          0     0   \n",
      "\n",
      "        wrong_fragment  urgent  hot  ...  dst_host_srv_count  \\\n",
      "0                    0       0    0  ...                  25   \n",
      "1                    0       0    0  ...                   1   \n",
      "2                    0       0    0  ...                  26   \n",
      "3                    0       0    0  ...                 255   \n",
      "4                    0       0    0  ...                 255   \n",
      "...                ...     ...  ...  ...                 ...   \n",
      "125968               0       0    0  ...                  25   \n",
      "125969               0       0    0  ...                 244   \n",
      "125970               0       0    0  ...                  30   \n",
      "125971               0       0    0  ...                   8   \n",
      "125972               0       0    0  ...                  77   \n",
      "\n",
      "        dst_host_same_srv_rate  dst_host_diff_srv_rate  \\\n",
      "0                         0.17                    0.03   \n",
      "1                         0.00                    0.60   \n",
      "2                         0.10                    0.05   \n",
      "3                         1.00                    0.00   \n",
      "4                         1.00                    0.00   \n",
      "...                        ...                     ...   \n",
      "125968                    0.10                    0.06   \n",
      "125969                    0.96                    0.01   \n",
      "125970                    0.12                    0.06   \n",
      "125971                    0.03                    0.05   \n",
      "125972                    0.30                    0.03   \n",
      "\n",
      "        dst_host_same_src_port_rate  dst_host_srv_diff_host_rate  \\\n",
      "0                              0.17                         0.00   \n",
      "1                              0.88                         0.00   \n",
      "2                              0.00                         0.00   \n",
      "3                              0.03                         0.04   \n",
      "4                              0.00                         0.00   \n",
      "...                             ...                          ...   \n",
      "125968                         0.00                         0.00   \n",
      "125969                         0.01                         0.00   \n",
      "125970                         0.00                         0.00   \n",
      "125971                         0.00                         0.00   \n",
      "125972                         0.30                         0.00   \n",
      "\n",
      "        dst_host_serror_rate  dst_host_srv_serror_rate  dst_host_rerror_rate  \\\n",
      "0                       0.00                      0.00                  0.05   \n",
      "1                       0.00                      0.00                  0.00   \n",
      "2                       1.00                      1.00                  0.00   \n",
      "3                       0.03                      0.01                  0.00   \n",
      "4                       0.00                      0.00                  0.00   \n",
      "...                      ...                       ...                   ...   \n",
      "125968                  1.00                      1.00                  0.00   \n",
      "125969                  0.00                      0.00                  0.00   \n",
      "125970                  0.72                      0.00                  0.01   \n",
      "125971                  1.00                      1.00                  0.00   \n",
      "125972                  0.00                      0.00                  0.00   \n",
      "\n",
      "        dst_host_srv_rerror_rate  label  \n",
      "0                           0.00      1  \n",
      "1                           0.00      1  \n",
      "2                           0.00      0  \n",
      "3                           0.01      1  \n",
      "4                           0.00      1  \n",
      "...                          ...    ...  \n",
      "125968                      0.00      0  \n",
      "125969                      0.00      1  \n",
      "125970                      0.00      1  \n",
      "125971                      0.00      0  \n",
      "125972                      0.00      1  \n",
      "\n",
      "[125973 rows x 41 columns]\n"
     ]
    }
   ],
   "source": [
    "def oh_encoder(train_df, test_df, nominal_features):\n",
    "    enc = OneHotEncoder()\n",
    "    train_encoded = enc.fit_transform(train_df[nominal_features]).toarray()\n",
    "    test_encoded = enc.transform(test_df[nominal_features]).toarray()\n",
    "    new_columns = []\n",
    "    for i, feature in enumerate(nominal_features):\n",
    "        new_columns.extend([f\"{feature}_{str(cat)}\" for cat in enc.categories_[i]])\n",
    "\n",
    "    train_ohe = train_df.drop(nominal_features, axis=1)\n",
    "    train_ohe = pd.concat([train_ohe, pd.DataFrame(train_encoded, columns=new_columns)], axis=1)\n",
    "\n",
    "    test_ohe = test_df.drop(nominal_features, axis=1)\n",
    "    test_ohe = pd.concat([test_ohe, pd.DataFrame(test_encoded, columns=new_columns)], axis=1)\n",
    "\n",
    "    return train_ohe, test_ohe\n",
    "\n",
    "def l_encoder(train_df, test_df, nominal_features):\n",
    "    enc = LabelEncoder()\n",
    "    for feature in nominal_features:\n",
    "        train_df[feature] = enc.fit_transform(train_df[feature])\n",
    "        test_df[feature] = enc.transform(test_df[feature])\n",
    "\n",
    "    return train_df, test_df\n",
    "\n",
    "train_new, test_new = l_encoder(train_df, test_df, ['protocol_type', 'service', 'flag', 'label'])\n",
    "print(train_new)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per le feature continue: Se queste seguono o sono vicine a una distribuzione normale, \n",
    "# lo StandardScaler è spesso una buona scelta. Se invece ci sono outlier o i dati \n",
    "# sono distribuiti in modo non normale, considera RobustScaler o MinMaxScaler. (CONTROLLARE GLI OUTLIERS)\n",
    "# Per le feature categoriali codificate (come One-Hot Encoded): in generale, \n",
    "# non è necessario applicare uno scaler poiché i valori saranno già binari (0 e 1). \n",
    "# Tuttavia, se hai usato un encoding come il Target Encoding, potresti voler applicare \n",
    "# uno scaler come MinMaxScaler per portare i valori target in un intervallo uniforme.\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "features_to_scale = ['duration', 'src_bytes', 'dst_bytes', 'count', 'srv_count', \n",
    "                     'serror_rate', 'rerror_rate', 'same_srv_rate', 'diff_srv_rate', \n",
    "                     'dst_host_count', 'dst_host_srv_count', 'dst_host_same_srv_rate',\n",
    "                     'dst_host_diff_srv_rate', 'dst_host_same_src_port_rate',\n",
    "                     'dst_host_serror_rate', 'dst_host_rerror_rate']\n",
    "\n",
    "# def scaler(train_df, test_df, numeric_features, scaler = MinMaxScaler()):\n",
    "#     train_scaled = scaler.fit_transform(train_df[numeric_features])\n",
    "#     test_scaled = scaler.transform(test_df[numeric_features])\n",
    "\n",
    "#     train_ss = train_df.drop(numeric_features, axis=1)\n",
    "#     train_ss = pd.concat([train_ss, pd.DataFrame(train_scaled, columns=numeric_features)], axis=1)\n",
    "\n",
    "#     test_ss = test_df.drop(numeric_features, axis=1)\n",
    "#     test_ss = pd.concat([test_ss, pd.DataFrame(test_scaled, columns=numeric_features)], axis=1)\n",
    "\n",
    "#     return train_ss, test_ss\n",
    "\n",
    "ss = MinMaxScaler()\n",
    "train_new[features_to_scale] = ss.fit_transform(train_new[features_to_scale])\n",
    "test_new[features_to_scale] = ss.transform(test_new[features_to_scale])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discretization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import KBinsDiscretizer\n",
    "\n",
    "# discretizer = KBinsDiscretizer(n_bins=5, encode='ordinal', strategy='uniform')\n",
    "# X_discretized = discretizer.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection \n",
    "\n",
    "1. Attribute Ratio \n",
    "\n",
    "Attribute Ratio approach is used for feature selection purposes. This approach was described by Hee-su Chae and Sang Hyun Choi in Feature Selection for efficient Intrusion Detection using Attribute Ratio and Feature Selection for Intrusion Detection using NSL-KDD\n",
    "\n",
    "This approach is also used for nominal variables as they were encoded as binary variables above.\n",
    "\n",
    "As it is a possible to have 'null' values because binary features could have Frequency(0) = 0, those 'null' values are replaced with 1000.0 (magic number). For NSL KDD dataset it is related only for 'protocol_type_tcp' ohe variable.\n",
    "\n",
    "In section 4, we explain NSL-KDD data which has three attribute types. We use attribute average and frequency for each class calculate the AR from numeric and binary type. AR can be calculated as :                             (7)   Class Ratio (CR) is attribute is ratio of each class for Attribute i. CR is calculated by two methods according to the type of attributes. CR can be calculated as for numeric :                                      (8)  CR can be calculated as for binary : \n",
    "\n",
    "After calculating AR(i), Features rank ordering larger AR. Table 4 shows the rank of features with a calculated AR. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ho calcolato l'attribute ratio per ogni feature (divisione in 5 classi)\n",
    "train_5_df = Dataset(train_df).get_label5()\n",
    "\n",
    "def attributeRatio(feature, dataset):\n",
    "    class_ratio = {}\n",
    "\n",
    "    if feature in nominal_features:\n",
    "        return 0\n",
    "\n",
    "    if feature in binary_features:\n",
    "        for label in dataset['label'].unique():\n",
    "            class_ones = dataset[dataset['label'] == label][feature].sum()\n",
    "            class_zeros = dataset[dataset['label'] == label][feature].count() - class_ones\n",
    "            class_ratio[label] = class_ones / class_zeros\n",
    "    else:\n",
    "        total_mean = dataset[feature].mean()\n",
    "        for label in dataset['label'].unique():\n",
    "            class_mean = dataset[dataset['label'] == label][feature].mean()\n",
    "            class_ratio[label] = class_mean / total_mean\n",
    "    return max(class_ratio.values())\n",
    "\n",
    "# calcola l'attribute ratio per ogni feature tranne 'label' e 'score'\n",
    "attribute_ratios = {}\n",
    "for feature in train_5_df.columns:\n",
    "    if feature not in ['label', 'score', 'num_outbound_cmds']:\n",
    "        attribute_ratios[feature] = attributeRatio(feature, train_5_df)\n",
    "\n",
    "# ordina le feature in base all'attribute ratio\n",
    "sorted_attribute_ratios = sorted(attribute_ratios.items(), key=lambda x: x[1], reverse=True)\n",
    "# stampa una tabella con le feature ordinate. la prima colonna è il rank, la seconda la feature e la terza l'attribute ratio\n",
    "for i, (feature, ratio) in enumerate(sorted_attribute_ratios):\n",
    "    print(f\"{i+1} & {feature} & {ratio:.2f} \\\\\\\\\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Filter methods\n",
    "\n",
    "https://medium.com/@sariq16/correlation-based-feature-selection-in-a-data-science-project-3ca08d2af5c6\n",
    "\n",
    "seleziona le k feature più rilevanti secondo il chi2 (hanno alta correlazione con il label e bassa con le altre feature), ANOVA F-test, Mutual Information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_features(train_data, test_data, score_func, k):\n",
    "    X = train_data.drop(['label'], axis=1)\n",
    "    y = train_data['label']\n",
    "    selector = SelectKBest(score_func=score_func, k = k).fit(X, y)\n",
    "    X_train_selected = selector.transform(X)\n",
    "    X_test_selected = selector.transform(test_data.drop(['label'], axis=1))\n",
    "    selected_features = X.columns[selector.get_support()]\n",
    "    print(selected_features)\n",
    "    return X_train_selected, X_test_selected\n",
    "\n",
    "def cfs(train_data, test_data):\n",
    "    X_train = train_data.drop(['label'], axis=1)\n",
    "    X_test = test_data.drop(['label'], axis=1)\n",
    "    corr_matrix = X_train.corr().abs()\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "    to_drop = [column for column in upper.columns if any(upper[column] > 0.9)]\n",
    "    X_train = X_train.drop(to_drop, axis=1)\n",
    "    X_test = X_test.drop(to_drop, axis=1)\n",
    "    print(X_train.columns)\n",
    "    return X_train, X_test\n",
    "\n",
    "k = 30\n",
    "train_reduced, test_reduced = get_best_features(train_new, test_new, f_classif, k)\n",
    "print(train_reduced.shape)\n",
    "train_reduced, test_reduced = get_best_features(train_new, test_new, chi2, k)\n",
    "print(train_reduced.shape)\n",
    "train_reduced, test_reduced = get_best_features(train_new, test_new, mutual_info_classif, k)\n",
    "print(train_reduced.shape)\n",
    "train_reduced, test_reduced = cfs(train_new, test_new)\n",
    "print(train_reduced.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Wrapper Methods\n",
    "\n",
    "Questi metodi iterano attraverso le combinazioni di feature e valutano la loro importanza usando un modello predittivo.\n",
    "\n",
    "Recursive Feature Elimination (RFE): Utilizza un modello per selezionare feature rilevanti, eliminando quelle meno importanti ad ogni iterazione. Funziona bene con modelli come SVM, alberi decisionali o regressioni logistiche."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['protocol_type', 'service', 'flag', 'count', 'same_srv_rate',\n",
      "       'diff_srv_rate', 'dst_host_srv_count', 'dst_host_diff_srv_rate',\n",
      "       'dst_host_same_src_port_rate', 'dst_host_serror_rate'],\n",
      "      dtype='object')\n",
      "(125973, 10)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def rfe(train_data, test_data, k):\n",
    "    X = train_data.drop(['label'], axis=1)\n",
    "    y = train_data['label']\n",
    "    X_test = test_data.drop(['label'], axis=1)\n",
    "\n",
    "    model = RandomForestClassifier()\n",
    "    rfe = RFE(model, \n",
    "              n_features_to_select=k).fit(X, y)\n",
    "    X_train_selected = rfe.transform(X)\n",
    "    X_test_selected = rfe.transform(X_test)\n",
    "    print(X.columns[rfe.get_support()])\n",
    "    return X_train_selected, X_test_selected\n",
    "k = 10\n",
    "train_reduced, test_reduced = rfe(train_new, test_new, k)\n",
    "print(train_reduced.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequential Feature Selection (SFS): Aggiunge o rimuove feature in modo sequenziale, valutando il modello in ogni passaggio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['duration', 'protocol_type', 'service', 'flag', 'src_bytes',\n",
      "       'dst_bytes', 'wrong_fragment', 'hot', 'logged_in', 'num_compromised',\n",
      "       'root_shell', 'su_attempted', 'num_root', 'num_file_creations',\n",
      "       'num_shells', 'num_access_files', 'is_host_login', 'is_guest_login',\n",
      "       'srv_count', 'rerror_rate', 'srv_rerror_rate', 'diff_srv_rate',\n",
      "       'srv_diff_host_rate', 'dst_host_count', 'dst_host_srv_count',\n",
      "       'dst_host_diff_srv_rate', 'dst_host_same_src_port_rate',\n",
      "       'dst_host_srv_diff_host_rate', 'dst_host_serror_rate',\n",
      "       'dst_host_rerror_rate'],\n",
      "      dtype='object')\n",
      "(125973, 30)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "def sfs(train_data, test_data, k):\n",
    "    X = train_data.drop(['label'], axis=1)\n",
    "    y = train_data['label']\n",
    "    X_test = test_data.drop(['label'], axis=1)\n",
    "\n",
    "    model = RandomForestClassifier()\n",
    "    sfs = SequentialFeatureSelector(model, \n",
    "                                    cv = StratifiedKFold(n_splits=5, random_state=123, shuffle=True),\n",
    "                                    scoring = 'accuracy', \n",
    "                                    direction='forward', \n",
    "                                    n_features_to_select=k).fit(X, y)\n",
    "    X_train_selected = sfs.transform(X)\n",
    "    X_test_selected = sfs.transform(X_test)\n",
    "    print(X.columns[sfs.get_support()])\n",
    "    return X_train_selected, X_test_selected\n",
    "\n",
    "train_reduced, test_reduced = sfs(train_new, test_new, k)\n",
    "print(train_reduced.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calcola information gain \n",
    "# def get_information_gain(dataset, feature):\n",
    "#     entropy = 0\n",
    "#     for label in dataset['label'].unique():\n",
    "#         p = dataset[dataset['label'] == label][feature].sum() / dataset[feature].sum()\n",
    "#         entropy += p * np.log2(p)\n",
    "#     return entropy\n",
    "\n",
    "# information_gains = {}\n",
    "# for feature in train_5_df.columns:\n",
    "#     if feature not in ['label', 'score', 'num_outbound_cmds']:\n",
    "#         information_gains[feature] = get_information_gain(train_5_df, feature)\n",
    "\n",
    "# # ordina le feature in base all'information gain\n",
    "# sorted_information_gains = sorted(information_gains.items(), key=lambda x: x[1], reverse=True)\n",
    "# # stampa una tabella con le feature ordinate. la prima colonna è il rank, la seconda la feature e la terza l'information gain\n",
    "# for i, (feature, gain) in enumerate(sorted_information_gains):\n",
    "#     print(f\"{i+1} & {feature} & {gain:.2f} \\\\\\\\\")\n",
    "\n",
    "\n",
    "# # calcola gain ratio\n",
    "# def get_gain_ratio(dataset, feature):\n",
    "#     split_info = 0\n",
    "#     for label in dataset['label'].unique():\n",
    "#         p = dataset[dataset['label'] == label][feature].sum() / dataset[feature].sum()\n",
    "#         split_info += p * np.log2(p)\n",
    "#     split_info = -split_info\n",
    "#     return information_gains[feature] / split_info\n",
    "\n",
    "# gain_ratios = {}\n",
    "# for feature in train_5_df.columns:\n",
    "#     if feature not in ['label', 'score', 'num_outbound_cmds']:\n",
    "#         gain_ratios[feature] = get_gain_ratio(train_5_df, feature)\n",
    "\n",
    "# # ordina le feature in base al gain ratio\n",
    "# sorted_gain_ratios = sorted(gain_ratios.items(), key=lambda x: x[1], reverse=True)\n",
    "# # stampa una tabella con le feature ordinate. la prima colonna è il rank, la seconda la feature e la terza il gain ratio\n",
    "# for i, (feature, ratio) in enumerate(sorted_gain_ratios):\n",
    "#     print(f\"{i+1} & {feature} & {ratio:.2f} \\\\\\\\\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def pca(train_data, test_data, k):\n",
    "    X = train_data.drop(['label'], axis=1)\n",
    "    X_test = test_data.drop(['label'], axis=1)\n",
    "\n",
    "    pca = PCA(n_components=k)\n",
    "    X_train_pca = pca.fit_transform(X)\n",
    "    X_test_pca = pca.transform(X_test)\n",
    "    \n",
    "    print(X_train_pca.shape)\n",
    "    print(pca.components_)\n",
    "    print(pca.explained_variance_ratio_)\n",
    "    print(pca.explained_variance_)\n",
    "    print(pca.singular_values_)\n",
    "    print(pca.singular_values_.sum())\n",
    "\n",
    "    return X_train_pca, X_test_pca"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envTesi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
